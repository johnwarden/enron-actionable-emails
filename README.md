This is an exercise in using machine learning to classify emails as "actionable" or "not actionable", using the Enron email corpus as training and test data: http://www.cs.cmu.edu/~enron/

## Running

The script run.sh will download the MySQL dump, load into a MySQL database, create the training/test data, train the model, and validate the model, outputting a whole bunch of information to STDERR/STDOUT.  You may need to uncomment the first two lines in run.sh for it to download/unzip the MySQL dump.

You need to have mysql5 installed and available in your path.  The code creates and works with a database 'enron' as user root, so you may be asked a few times for your mysql root password a few times.  A copy of the Stanford Classifier distribution has been checked in here, so that doesn't need to be installed.

## Creating Training Data

There is a MySQL dump of the enron corpus available here:

  https://s3.amazonaws.com/rjurney_public_web/images/enron.mysql.5.5.20.sql.gz

And described here

  http://hortonworks.com/blog/the-data-lifecycle-part-one-avroizing-the-enron-emails/

After loading this into a mysql5 database, create-training-data.sql will use a SQL join to calculate the **replier ratio** for each email.  The replier ratio is (# of recipients)/(# of recipients who responded at least once).  Identifying a reply to an email is not an exact science, as it relies on conventions for subject lines that people don't always fallow, so this output isn't perfect.

I've decided to use a somewhat arbitrary replier ratio threshhold of .1 to label emails as ACTIONABLE.  That is, if at least some of the recipients responded to an email, than presumably it was actionable for someone!  Probably pretty inaccurate, but it's the best way I could think of to create training data without manually labeling emails as actionable/not, which I didn't feel like doing.

`create-training-data.sql` splits emails into four buckets.  I use one to train the model, one to test/verify the model, and have reserved the other two for later iterations.  The test/training data is output to: `/tmp/actionable-emails.train` and `/tmp/actionable-emails.test`.

The training/test data is a tab-separated text file with no header.  The first column is the category (ACTIONABLE or NOT), the second is the subject, third the body, and forth the sender ID from the MySQL database (unique for each email).

## Training and Testing Model

The Stanford Classifier has a very handy commandline interface, and is very convenient if you are working with tab-separated text data.  Once you have your data files, a simple config file tells the classifier which column is the category, and how to extract word- or character- n-grams to use as features.

http://nlp.stanford.edu/software/classifier.shtml

All the settings are defined in the config file actionable-emails.prop.  The documentation for these settings is here:

http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/ColumnDataClassifier.html

I have configured the classifier to split the subject (column 1) into lower-case word n-grams and use each of these as a feature.  Because of some memory issues, which I didn't have time to fix, the config file ignores column 2 (the body).  It also uses column 3 (sender ID) as a feature.

It then trains a max-entropy model with the training data file, tests it against the test data file, and outputs some conclusions.

## Interpreting the Output

The final output looks like this:

	57050 examples in test set
	Cls NOT: TP=42955 FN=4204 FP=6248 TN=3643; Acc 0.817 P 0.873 R 0.911 F1 0.892
	Cls ACTIONABLE: TP=3643 FN=6248 FP=4204 TN=42955; Acc 0.817 P 0.464 R 0.368 F1 0.411
	Micro-averaged accuracy/F1: 0.81679
	Macro-averaged F1: 0.65115

TP, FN, FP, TN means true positive, false negative, etc.  P and R are precision and recall, F1  is F1.

The output also includes detailed output, showing the subject, the label from the test data file (the "gold" label), and the label generated by the model, so you can see which emails were labeled correctly and which were labeled incorrectly (again, "correct" as determined by the training data, which itself may not be correct.).

The F1 score for NOT is .892, which is not bad.  But our data isn't balanced, so the classifier ended up being very skewed towards classifying everything as NOT actionable.  The F1 score for actionable is only .411.

Maco-averaged accuracy when validating the model against the *training* data was over .9, but against the test data was only .65.  This indicates some over-fitting going on here.

## Analyzing the Model

If you scroll way up above the detailed ouptut, you can see the top features of the model that was generated.  To interpret this, something like `(3-LSW-27237,ACTIONABLE)` is telling us that the feature '27237' (which is a senderid from the mysql database) extracted from column 3 (LSW means columns 3 was split on lower-case words) was associated with the ACTIONABLE category with a weight of 4.0675.  Basically, it means people tend to respond to this persons emails.  You can see about half of hte top-weighted features were senders, half were n-grams from the subject (christmas dinner, fw: severence, status needed, etc.).  Some of these are counter-intuitive and I suspect just an artifact of overfitting

	(3-LSW-27237,ACTIONABLE)               4.0675
	(3-LSW-50503,ACTIONABLE)               3.9851
	(3-LSW-57419,ACTIONABLE)               3.8471
	(3-LSW-24229,ACTIONABLE)               3.7402
	(3-LSW-29585,ACTIONABLE)               3.2732
	(1-SW#-budget-spreadsheet,ACTIONABLE)  3.2278
	(1-SW#-enron-complaint,ACTIONABLE)     3.0886
	(1-SW#-fw-entex,ACTIONABLE)            2.9392
	(1-SW#-study-update,ACTIONABLE)        2.9274
	(1-SW#-christmas-dinner,ACTIONABLE)    2.8758
	(3-LSW-39933,ACTIONABLE)               2.8297
	(3-LSW-56132,ACTIONABLE)               2.8242
	(1-SW#-ppl-corporation,ACTIONABLE)     2.8148
	(1-SW#-fw-severance,ACTIONABLE)        2.8136
	(1-SW#-fw-midland,ACTIONABLE)          2.7773
	(1-SW#-status-needed,ACTIONABLE)       2.7746
	(3-LSW-21297,ACTIONABLE)               2.7582
	(3-LSW-3592,NOT)                       2.7437
	(1-SW#-trading-inc.,ACTIONABLE)        2.7264
	(1-SW#-amerex-letter,ACTIONABLE)       2.7108
	(3-LSW-3563,ACTIONABLE)                2.6550
	(1-SW#-amendment-document,ACTIONABLE)  2.6401
	(1-SW#-coal-information,ACTIONABLE)    2.6322
	(1-SW#-possible-meeting,NOT)           2.6282
	(1-SW#-fw-port,ACTIONABLE)             2.6229
	(1-SW#-storage-schedules,ACTIONABLE)   2.5901
	(1-SW#-bpa-cuts,ACTIONABLE)            2.5744
	(1-SW#-the-election,ACTIONABLE)        2.5649
	(1-SW#-oneok-gty,ACTIONABLE)           2.5627
	(1-SW#-tiger-team,ACTIONABLE)          2.5596

## Conclusion

So results aren't steller but they do seem to show at least some predictive power using this model -- especially consider that the training data itself was not hand-tagged.  Better entity extraction (replacing these n-grams that are mostly noise with more meaningful text analysis), including features from the email body, fine-tuning the identification of "replies", fine tuning the Classifier parameters, and perhaps hand-tagging a training set would certainly increase the F1 score.
